from util import abstract_method
from util import Find, DataEndException
class Data(object):
    """abstract base class for data. Data class deals with any implementation
    details of the data. it can be a file, a sql data base, and so on, as long
    as it support the pure virtual methods defined here.
    """
    def get_fea_slice(self, rg=None, rg_type=None):
        """ get a slice of feature
        - **rg** is the range for the slice
        - **rg_type** is the type for range, it can be ['flow'|'time']
        """
        abstract_method()

    def get_max(self, fea, rg=None, rg_type=None):
        """ get the max value of feature during a range
        - **fea** is a list of feature name
        - **rg** is the range
        - **rg_type** is the range type
        the output is the a list of element which contains the max
        value of the feature in **fea**
        """
        abstract_method()
    def get_min(self, fea, rg=None, rg_time=None):
        """ get min value of feature within a range. see **get_max** for
        the meaning of the parameters
        """
        abstract_method()

##############################################################
####     For Hard Disk File Generated by fs-simulator   ######
##############################################################
from DataParser import RawParseData
class PreloadHardDiskFile(Data):
    """ for hard Disk File Generated by fs-simulator"""
    def __init__(self, f_name):
        """ data_order can be flow_first | feature_first
        """
        self.f_name = f_name
        self._init()

    def _init(self):
        self.fea_vec, self.fea_name = RawParseData(self.f_name)
        # import pdb;pdb.set_trace()
        self.zip_fea_vec = None
        self.t = [ float(t) for t in self._get_value_list('start_time')]
        self.min_time = min(self.t)
        self.max_time = max(self.t)
        self.flow_num = len(self.t)

    def _get_fea_idx(self, key):
        return self.fea_name.index(key)

    def _get_value_list(self, key):
        """get a value list for a single key"""
        # fidx = self.fea_name.index(key)
        fidx = self._get_fea_idx(key)
        self.zip_fea_vec = self.zip_fea_vec if self.zip_fea_vec else zip(*self.fea_vec)
        return self.zip_fea_vec[fidx]

    def _get_where(self, rg=None, rg_type=None):
        """get the absolute position of flows records that within the range.
        some times this function is called by outside function thus break
        the interoperability"""
        if not rg: return 0, self.flow_num-1
        if rg_type == 'flow':
            sp, ep = rg
            if sp >= self.flow_num: raise DataEndException()
        elif rg_type == 'time':
            sp = Find(self.t, rg[0]+self.min_time)
            ep = Find(self.t, rg[1]+self.min_time)
            # if rg[1] + self.min_time > self.max_time :
                # import pdb;pdb.set_trace()
                # raise Exception('Probably you set wrong range for normal flows? Go to check DETECTOR_DESC')

            assert(sp != -1 and ep != -1)
            if (sp == len(self.t)-1 or ep == len(self.t)-1):
                # import pdb;pdb.set_trace()
                raise DataEndException()
        else:
            raise ValueError('unknow window type')
        return sp, ep


    def get_fea_slice(self, fea=None, rg=None, rg_type=None, data_order='flow_first'):
        """this function is to get a chunk of feature vector.
        The feature belongs flows within the range specified by **rg**
        **rg_type** can be ['flow' | 'time' ].
        data_order can be flow_first | feature_first
        """
        sp, ep = self._get_where(rg, rg_type)
        if fea:
            fea_idx = [self.fea_name.index(f) for f in fea]
            if data_order == 'flow_first':
                return [[ v[i] for i in fea_idx] for v in self.fea_vec[sp:ep]]
            elif data_order == 'feature_first':
                return [self._get_value_list(f)[sp:ep] for f in fea]

        if data_order == 'flow_first':
            return self.fea_vec[sp:ep]
        elif data_order == 'feature_first':
            return [fv[sp:ep] for fv in self.zip_fea_vec]

    def get(self, handler, fea, rg, rg_type):
        """ get value list for each feature within the range and return
        the output of handler function in this value.
        **handler** is a function handler that take a list as input
        """
        fea = fea if fea else self.fea_name
        sl = self.get_fea_slice(fea, rg, rg_type)
        zip_sl = zip(*sl)
        res = []
        i = -1
        for f in fea:
            i += 0
            res.append( handler( zip_sl[i] ) )
        return res

    def get_max(self, fea, rg=None, rg_type=None):
        def max_fea(l):
            return max(float(v) for v in l )
        return self.get(max_fea, fea, rg, rg_type)
        # return [max(self._get_value_list(f)[sp:ep]) for f in fea]
        # return [max(float(val) for val in self._get_value_list(f)[sp:ep]) for f in fea]

    def get_min(self, fea, rg=None, rg_type=None):
        def min_fea(l):
            return min(float(v) for v in l )
        return self.get(min_fea, fea, rg, rg_type)
        # sp, ep = self._get_where(rg, rg_type)
        # fea = fea if fea else self.fea_name
        # import pdb;pdb.set_trace()
        # return [min(self._get_value_list(f)[sp:ep]) for f in fea]
        # return [min(float(val) for val in self._get_value_list(f)[sp:ep]) for f in fea]


##############################################################
####  For Hard Disk File Generated by pcap2netflow tool,######
####  for more information about pcap2netflow, please   ######
####  visit https://bitbucket.org/hbhzwj/pcap2netflow   ######
##############################################################
import re
def get_ip_port(val):
    ip_tmp, port = val.rsplit(':')
    ip = ip_tmp[1:-1]
    return ip, port

def ParseData_pcap2netflow(fileName):
    """
    the input is the filename of the flow file that needs to be parsed.
    the ouput is list of dictionary contains the information for each flow in the data. all these information are strings, users need
    to tranform them by themselves
    """
    flow = []
    # FORMAT = dict(start_time=3, end_time=4, src_ip=5, sc_port=6, octets=13, ) # Defines the FORMAT of the data file
    fid = open(fileName, 'r')
    while True:
        line = fid.readline()
        if not line or not line.startswith('FLOW'):
            break
        if line == '\n': # Ignore Blank Line
            continue
        item = re.split('[ ]', line) #FIXME need to be changed if want to use port information
        f = dict()
        for i in xrange(1, len(item)-1, 2):
            f[item[i]] = item[i+1]
        src_ip, src_port = get_ip_port(f['src'])
        dst_ip, dst_port = get_ip_port(f['dst'])
        f['src_ip'] = src_ip
        f['src_port'] = src_port
        f['dst_ip'] = dst_ip
        f['dst_port'] = dst_port
        f['flow_size'] = f['octets']

        flow.append(f.values())
    fid.close()

    if not flow: raise Exception('Not even a flow is found. Are you specifying the right filename?')

    return flow, f.keys()

class PreloadHardDiskFile_pcap2netflow(PreloadHardDiskFile):
    """data with format of pcap2netflow, (softflowd and flowd-reader)
    """
    def _init(self):
        self.fea_vec, self.fea_name = ParseData_pcap2netflow(self.f_name)
        self.zip_fea_vec = None
        self.flow_num = len(self.fea_vec)

    def _get_where(self, rg=None, rg_type=None):
        if rg_type != 'flow':
            import sys
            print """[Error]: data of pcap2netflow format can only use flow window,
please change your window type to 'flow'"""
            sys.exit()
        else:
            super(PreloadHardDiskFile_pcap2netflow, self)._get_where(rg, rg_type)


""" Data File Generated By Flow Exporter tool
"""
def ParseFlowExporterData(f_name):
    """parse the data format generated by FlowExporter.py tool
    """
    # raw = lambda x:x
    # dotted_int = lambda x: tuple(int(v) for v in x.rsplit('.'))
    def ip_str(x):
        if x.upper() == 'BROADCAST':
            return '255.255.255.255'
        elif len(x.rsplit('.')) != 4: # not ip v4
            return '0.0.0.0' # return an arbitracy address
        else:
            return x

    FORMAT = dict(
            start_time = (0, float),
            src_ip = (1, ip_str),
            dst_ip = (2, ip_str),
            protocol = (3, str),
            flow_size = (4, float),
            flow_dur = (5, float),
            )


    fid = open(f_name, 'r')
    record = []
    while True:
        tline = fid.readline()
        if not tline:
            break
        if tline == '\n': # Ignore Blank Line
            continue
        item = tline.rsplit()
        f = [fmter(item[v]) for (v, fmter) in FORMAT.itervalues()]
        record.append(f)

    fid.close()
    return record, FORMAT.keys()

class PreloadHardDiskFile_FlowExporter(PreloadHardDiskFile):
    def _init(self):
        self.fea_vec, self.fea_name = ParseFlowExporterData(self.f_name)
        self.zip_fea_vec = zip(*self.fea_vec)
        self.t = self.zip_fea_vec[self.fea_name.index('start_time')]
        self.min_time = min(self.t)
        self.flow_num = len(self.fea_vec)


##############################################################
####  For simpleweb.org labled dataset, it is stored in ######
####  mysql server.                                     ######
####  visit http://www.simpleweb.org/wiki/Traces for    ######
####  more information (trace 8)                        ######
##############################################################

# from util import _mysql
from util import mysql
# try:
#     import _mysql
# except ImportError as e:
#     print '--> [warning] cannot import sql related function, \
#             reading for sql server is not supported'
    # print '--> e:', e

get_sec_msec = lambda x: [int(x), int( (x-int(x)) * 1e3)]

class SQLFile_SperottoIPOM2009(Data):
    """Data File wrapper for SperottoIPOM2009 format. it is store in mysql server, visit
     http://traces.simpleweb.org/traces/netflow/netflow2/dataset_description.txt
    """
    def __init__(self, spec):
        # self.db = _mysql.connect(**spec)
        self.db = mysql.connect(**spec)
        self._init()

    def _init(self):
        # select minimum time
        self.db.query("""SELECT start_time, start_msec FROM flows WHERE (id = 1);""")
        r = self.db.store_result()
        self.min_time_tuple = r.fetch_row()[0]
        self.min_time = float("%s.%s"%self.min_time_tuple)

        self.db.query("""SELECT MAX(id) FROM flows;""")
        r = self.db.store_result()
        self.flow_num = int(r.fetch_row()[0][0])

        self.db.query("""SELECT end_time, end_msec FROM flows WHERE (id = %d);"""%(self.flow_num))
        r = self.db.store_result()

        self.max_time_tuple = r.fetch_row()[0]
        self.max_time = float("%s.%s"%self.max_time_tuple)

    def _get_sql_where(self, rg=None, rg_type=None):
        if rg:
            if rg_type == 'flow':
                SQL_SEN_WHERE = """ WHERE ( (id >= %f) AND (id < %f) )""" %tuple(rg)
                if rg[0] > self.flow_num:
                    raise DataEndException("reach data end")

            elif rg_type == 'time':
                st = get_sec_msec (rg[0] + self.min_time)
                ed = get_sec_msec (rg[1] + self.min_time)
                SQL_SEN_WHERE = """ WHERE ( (start_time > %d) OR ( (start_time = %d) AND (start_msec >= %d)) ) AND
                             ( (end_time < %d) OR ( (end_time = %d) and (end_msec < %d) ) )""" %(st[0], st[0], st[1], ed[0], ed[0], ed[1])

                # print 'rg[0]', rg[0]
                # print 'self.min_time', self.min_time
                # print 'current time, ', rg[0] + self.min_time
                # print 'self.maxtime', self.max_time
                if rg[0] + self.min_time > self.max_time:
                    raise DataEndException("reach data end")
            else:
                print 'rg_type', rg_type
                raise ValueError('unknow window type')
        else:
            SQL_SEN_WHERE = ""
        return SQL_SEN_WHERE

    def get_max(self, fea, rg=None, rg_type=None):
        fea_str = ['MAX(%s)'%(f) for f in fea]
        SQL_SEN = """SELECT %s FROM flows"""%(",".join(fea_str)) + self._get_sql_where(rg, rg_type) + ";"
        self.db.query(SQL_SEN)
        r = self.db.store_result().fetch_row(0)
        return r[0]

    def get_min(self, fea, rg=None, rg_type=None):
        fea_str = ['MIN(%s)'%(f) for f in fea]
        SQL_SEN = """SELECT %s FROM flows"""%(",".join(fea_str)) + self._get_sql_where(rg, rg_type) + ";"
        self.db.query(SQL_SEN)
        r = self.db.store_result().fetch_row(0)
        return r[0]

    def get_fea_slice(self, fea, rg=None, rg_type=None):
        """this function is to get a chunk of feature vector.
        The feature belongs flows within the range specified by **rg**
        **rg_type** can be ['flow' | 'time' ].
        """
        SQL_SEN = """SELECT %s FROM flows"""%(",".join(fea)) + self._get_sql_where(rg, rg_type) + ";"
        # print SQL_SEN
        self.db.query(SQL_SEN)
        result = self.db.store_result().fetch_row(0)
        # return [line[0] for line in result] if len(fea) == 1 else result
        return result


#######################################################
## Flow Records Generated by xflow tools            ###
#######################################################
from time import strptime, mktime
def argsort(seq):
    # http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python
    return sorted(range(len(seq)), key=seq.__getitem__)

from util import argsort

class PreloadHardDiskFile_xflow(PreloadHardDiskFile):
    attr_mapping = { # define the synonym
            'src_ip': 'client_ip',
            'flow_size': 'Cb',
            }

    def _init(self):
        self.fea_vec, self.fea_name = self.parse_xflow(self.f_name)
        self.zip_fea_vec = None
        self.flow_num = len(self.fea_vec)
        self.zip_fea_vec = None
        self.t = [ float(t) for t in self._get_value_list('start_time')]
        self.min_time = min(self.t)
        self.max_time = max(self.t)
        self.flow_num = len(self.t)

    def get_fea_slice(self, fea=None, rg=None, rg_type=None, data_order='flow_first'):
        fea = [self.attr_mapping.get(v, v) for v in fea]# synonym replacement
        return PreloadHardDiskFile.get_fea_slice(self, fea, rg, rg_type, data_order)

    def _get_fea_idx(self, key):
        key = self.attr_mapping.get(key, key)
        return self.fea_name.index(key)

    @staticmethod
    def parse_xflow(fileName):
        """
        the input is the filename of the flow file that needs to be parsed.
        the ouput is list of dictionary contains the information for each flow in the data. all these information are strings, users need
        to tranform them by themselves
        """
        flow = []
        # Defines the FORMAT of the data file
        FORMAT = dict()
        FORMAT[11] = dict(
            start_time=0,
            proto=2,
            client_ip=3,
            direction=4,
            server_ip=5,
            Cb=7,
            Cp=8,
            Sb=9,
            Sp=10,
            )
        FORMAT[12] = dict(
            start_time=0,
            proto=3,
            client_ip=4,
            direction=5,
            server_ip=6,
            Cb=8,
            Cp=9,
            Sb=10,
            Sp=11,
            )
        FORMAT[13] = dict(
            start_time=0,
            proto=2,
            client_ip=3,
            # client_port=4,
            direction=5,
            server_ip=6,
            # server_port=7,
            Cb=9,
            Cp=10,
            Sb=11,
            Sp=12,
            )
        FORMAT[14] = dict(
            start_time=0,
            proto=3,
            client_ip=4,
            # client_port=5,
            direction=6,
            server_ip=7,
            # server_port=8,
            Cb=10,
            Cp=11,
            Sb=12,
            Sp=13,
            )
        # dotted_to_int = lambda x: [int(val) for val in x.rsplit('.')]
        port_str_to_int = lambda x: int(x[1:])
        attr_convert = lambda x: float( x.rsplit('=')[1].rsplit(',')[0] )
        handler = dict(
                start_time = lambda x: mktime(strptime(x, '%Y%m%d.%H:%M:%S')),
                proto = lambda x: x,
                # client_ip = dotted_to_int,
                client_ip = lambda x: x,
                client_port=port_str_to_int,
                direction=lambda x: x,
                # server_ip=dotted_to_int,
                server_ip= lambda x:x,
                server_port=port_str_to_int,
                Cb= attr_convert,
                Cp= attr_convert,
                Sb= attr_convert,
                Sp= lambda x: float(x.rsplit('=')[1].rsplit('\n')[0]),
                )
        fea_name = FORMAT[13].keys()
        fid = open(fileName, 'r')
        t = []
        while True:
            line = fid.readline()
            if not line: break
            item = re.split(' ', line)
            try:
                f = [handler[k](item[v]) for k,v in FORMAT[len(item)].iteritems()]
            except KeyError:
                raise Exception('Unexpected Flow Data Format')
            t.append(f[fea_name.index('start_time')])
            flow.append(f)
        fid.close()

        arg_t = argsort(t)
        sort_flow = [flow[i] for i in arg_t ]

        return sort_flow, fea_name



